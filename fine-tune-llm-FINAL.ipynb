{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-12-26T08:23:44.341487Z",
     "iopub.status.busy": "2024-12-26T08:23:44.341251Z",
     "iopub.status.idle": "2024-12-26T08:26:24.678576Z",
     "shell.execute_reply": "2024-12-26T08:26:24.677673Z",
     "shell.execute_reply.started": "2024-12-26T08:23:44.341467Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install Pytorch\n",
    "%pip install \"torch==2.2.2\" tensorboard\n",
    "\n",
    "# Install Hugging Face libraries\n",
    "%pip install  --upgrade \"transformers==4.40.0\" \"datasets==2.18.0\" \"accelerate==0.29.3\" \"evaluate==0.4.1\" \"bitsandbytes==0.43.1\" \"huggingface_hub==0.22.2\" \"trl==0.8.6\" \"peft==0.10.0\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-12-26T08:10:27.012263Z",
     "iopub.status.busy": "2024-12-26T08:10:27.011907Z",
     "iopub.status.idle": "2024-12-26T08:10:27.710848Z",
     "shell.execute_reply": "2024-12-26T08:10:27.710038Z",
     "shell.execute_reply.started": "2024-12-26T08:10:27.012238Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T08:10:31.311035Z",
     "iopub.status.busy": "2024-12-26T08:10:31.310730Z",
     "iopub.status.idle": "2024-12-26T08:10:31.445042Z",
     "shell.execute_reply": "2024-12-26T08:10:31.443929Z",
     "shell.execute_reply.started": "2024-12-26T08:10:31.311011Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git config --global credential.helper store  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-12-26T08:10:33.123480Z",
     "iopub.status.busy": "2024-12-26T08:10:33.123141Z",
     "iopub.status.idle": "2024-12-26T08:10:36.462277Z",
     "shell.execute_reply": "2024-12-26T08:10:36.461399Z",
     "shell.execute_reply.started": "2024-12-26T08:10:33.123454Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install peft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T08:10:41.332558Z",
     "iopub.status.busy": "2024-12-26T08:10:41.332253Z",
     "iopub.status.idle": "2024-12-26T08:10:54.094648Z",
     "shell.execute_reply": "2024-12-26T08:10:54.093989Z",
     "shell.execute_reply.started": "2024-12-26T08:10:41.332536Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import functools\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, balanced_accuracy_score, accuracy_score\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T08:10:58.077216Z",
     "iopub.status.busy": "2024-12-26T08:10:58.076542Z",
     "iopub.status.idle": "2024-12-26T08:10:59.993830Z",
     "shell.execute_reply": "2024-12-26T08:10:59.993025Z",
     "shell.execute_reply.started": "2024-12-26T08:10:58.077181Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the uploaded file\n",
    "file_path = '/kaggle/input/sentiment-a3/selected_comments.xlsx'\n",
    "\n",
    "# Load the Excel file into a Pandas DataFrame\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "df['entities']=df['entities'].astype('category')\n",
    "df['target']=df['entities'].cat.codes\n",
    "\n",
    "df.head()\n",
    "\n",
    "df=df[12500:]\n",
    "len(df)\n",
    "\n",
    "\n",
    "## We will be creaing 9 batches of 512 size for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T08:11:02.190299Z",
     "iopub.status.busy": "2024-12-26T08:11:02.189661Z",
     "iopub.status.idle": "2024-12-26T08:11:02.198645Z",
     "shell.execute_reply": "2024-12-26T08:11:02.197815Z",
     "shell.execute_reply.started": "2024-12-26T08:11:02.190268Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df['entities'].cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T08:11:04.080711Z",
     "iopub.status.busy": "2024-12-26T08:11:04.080392Z",
     "iopub.status.idle": "2024-12-26T08:11:04.086610Z",
     "shell.execute_reply": "2024-12-26T08:11:04.085730Z",
     "shell.execute_reply.started": "2024-12-26T08:11:04.080684Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "category_map = {code: category for code, category in enumerate(df['entities'].cat.categories)}\n",
    "category_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into train/val/test for later comparison.\n",
    "## For simplicity we split based on time.\n",
    "### First 60% train\n",
    "### Next 20% val\n",
    "### Next 20% test\n",
    "#### This can be problematic a bit since class balance changes over time and some articles on boundries between train/val or val/test have some overlap, but completely beats bias of stratified sample usually used since some articles are literally on same thing, but maybe different sources.<br>\n",
    "#### Essentially one need to ensure that the data in train and val and test are pretty independent therefore one cannot use stratified since that would give random spread probably giving the differentr sources of same news into train , val, test. Thus curropting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T08:11:07.239024Z",
     "iopub.status.busy": "2024-12-26T08:11:07.238706Z",
     "iopub.status.idle": "2024-12-26T08:11:07.246126Z",
     "shell.execute_reply": "2024-12-26T08:11:07.245470Z",
     "shell.execute_reply.started": "2024-12-26T08:11:07.238965Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Generalized split for train (60%), val (20%), and test (20%)\n",
    "\n",
    "# df= df[10000:]\n",
    "\n",
    "# Calculate the split indices\n",
    "train_split = int(0.6 * len(df))  # First 60%\n",
    "val_split = int(0.8 * len(df))    # Next 20% (60% + 20%)\n",
    "\n",
    "# Perform the splits\n",
    "df_train = df.iloc[:train_split, :]  # Train set\n",
    "df_val = df.iloc[train_split:val_split, :]  # Validation set\n",
    "df_test = df.iloc[val_split:, :]  # Test set\n",
    "\n",
    "# Print the shapes of the splits\n",
    "print(\"Train shape:\", df_train.shape)\n",
    "print(\"Validation shape:\", df_val.shape)\n",
    "print(\"Test shape:\", df_test.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PANDAS DATAFRAME TO HUGGINGFACE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T08:11:09.497617Z",
     "iopub.status.busy": "2024-12-26T08:11:09.497327Z",
     "iopub.status.idle": "2024-12-26T08:11:09.560549Z",
     "shell.execute_reply": "2024-12-26T08:11:09.559909Z",
     "shell.execute_reply.started": "2024-12-26T08:11:09.497598Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Converting pandas DataFrames into Hugging Face Dataset objects:\n",
    "dataset_train = Dataset.from_pandas(df_train.drop('entities',axis=1))\n",
    "dataset_val = Dataset.from_pandas(df_val.drop('entities',axis=1))\n",
    "dataset_test = Dataset.from_pandas(df_test.drop('entities',axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASET SHUFFLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T08:11:11.861149Z",
     "iopub.status.busy": "2024-12-26T08:11:11.860759Z",
     "iopub.status.idle": "2024-12-26T08:11:11.877678Z",
     "shell.execute_reply": "2024-12-26T08:11:11.877104Z",
     "shell.execute_reply.started": "2024-12-26T08:11:11.861120Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_train_shuffled = dataset_train.shuffle(seed=42)  # Using a seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASET DICTIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T08:11:14.038113Z",
     "iopub.status.busy": "2024-12-26T08:11:14.037807Z",
     "iopub.status.idle": "2024-12-26T08:11:14.043801Z",
     "shell.execute_reply": "2024-12-26T08:11:14.042938Z",
     "shell.execute_reply.started": "2024-12-26T08:11:14.038091Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = DatasetDict({\n",
    "    'train': dataset_train_shuffled,\n",
    "    'val': dataset_val,\n",
    "    'test': dataset_test\n",
    "})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASS WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T08:11:18.957485Z",
     "iopub.status.busy": "2024-12-26T08:11:18.957161Z",
     "iopub.status.idle": "2024-12-26T08:11:18.970124Z",
     "shell.execute_reply": "2024-12-26T08:11:18.969341Z",
     "shell.execute_reply.started": "2024-12-26T08:11:18.957463Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class_weights=(1/df_train.target.value_counts(normalize=True).sort_index()).tolist()\n",
    "class_weights=torch.tensor(class_weights)  # expects a python list not a numpy array!!\n",
    "class_weights=class_weights/class_weights.sum()\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL NAME -  \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "#### Meta-LLaMA: \n",
    "It is likely based on a LLaMA model architecture.\n",
    "#### 3-8B: \n",
    "The model might belong to the 3rd iteration (hypothetical) or is labeled as \"3\" for some specific reason, and it has 8 billion parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T08:11:30.547159Z",
     "iopub.status.busy": "2024-12-26T08:11:30.546802Z",
     "iopub.status.idle": "2024-12-26T08:18:40.412512Z",
     "shell.execute_reply": "2024-12-26T08:18:40.411701Z",
     "shell.execute_reply.started": "2024-12-26T08:11:30.547129Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the tokenizer\n",
    "\n",
    "### Since LLAMA3 pre-training doesn't have PAD token\n",
    "\n",
    "* Set the pad_token_id to eos_token_id\n",
    "* Set pad token ot eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T08:19:05.374758Z",
     "iopub.status.busy": "2024-12-26T08:19:05.374154Z",
     "iopub.status.idle": "2024-12-26T08:19:06.791946Z",
     "shell.execute_reply": "2024-12-26T08:19:06.791049Z",
     "shell.execute_reply.started": "2024-12-26T08:19:05.374694Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINE TUNING OF LLAMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAINER COMPONENTS\n",
    "* model\n",
    "* tokenizer\n",
    "* training arguments\n",
    "* train dataset\n",
    "* eval dataset\n",
    "* Data Collater\n",
    "* Compute Metrics\n",
    "* class_weights: In our case since we are using a custom trainer so we can use a weighted loss we will subclass trainer and define the custom loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create LLAMA tokenized dataset which will house our train/val parts during the training process but after applying tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T08:19:18.172009Z",
     "iopub.status.busy": "2024-12-26T08:19:18.171703Z",
     "iopub.status.idle": "2024-12-26T08:19:19.003374Z",
     "shell.execute_reply": "2024-12-26T08:19:19.002516Z",
     "shell.execute_reply.started": "2024-12-26T08:19:18.171962Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "col_to_delete = ['created_at', 'body']\n",
    "\n",
    "def llama_preprocessing_function(examples):\n",
    "    return tokenizer(examples['body'], truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "tokenized_datasets = dataset.map(llama_preprocessing_function, batched=True, remove_columns=col_to_delete)\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"target\", \"label\")\n",
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collator\n",
    "A **data collator** prepares batches of data for training or inference in machine learning, ensuring uniform formatting and adherence to model input requirements. This is especially crucial for variable-sized inputs like text sequences.\n",
    "\n",
    "### Functions of Data Collator\n",
    "\n",
    "1. **Padding:** Uniformly pads sequences to the length of the longest sequence using a special token, allowing simultaneous batch processing.\n",
    "2. **Batching:** Groups individual data points into batches for efficient processing.\n",
    "3. **Handling Special Tokens:** Adds necessary special tokens to sequences.\n",
    "4. **Converting to Tensor:** Transforms data into tensors, the required format for machine learning frameworks.\n",
    "\n",
    "### `DataCollatorWithPadding`\n",
    "\n",
    "The `DataCollatorWithPadding` specifically manages padding, using a tokenizer to ensure that all sequences are padded to the same length for consistent model input.\n",
    "\n",
    "- **Syntax:** `collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)`\n",
    "- **Purpose:** Automatically pads text data to the longest sequence in a batch, crucial for models like BERT or GPT.\n",
    "- **Tokenizer:** Uses the provided `tokenizer` for sequence processing, respecting model-specific vocabulary and formatting rules.\n",
    "\n",
    "This collator is commonly used with libraries like Hugging Face's Transformers, facilitating data preprocessing for various NLP models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T08:19:21.979998Z",
     "iopub.status.busy": "2024-12-26T08:19:21.979655Z",
     "iopub.status.idle": "2024-12-26T08:19:21.983562Z",
     "shell.execute_reply": "2024-12-26T08:19:21.982759Z",
     "shell.execute_reply.started": "2024-12-26T08:19:21.979947Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COMPUTE METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T08:19:24.139584Z",
     "iopub.status.busy": "2024-12-26T08:19:24.139287Z",
     "iopub.status.idle": "2024-12-26T08:19:24.143691Z",
     "shell.execute_reply": "2024-12-26T08:19:24.142872Z",
     "shell.execute_reply.started": "2024-12-26T08:19:24.139561Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {'balanced_accuracy' : balanced_accuracy_score(predictions, labels),'accuracy':accuracy_score(predictions,labels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Plan\n",
    "The plan is designed to train a model sequentially on batches of data, due to GPU memory constraints. The idea is to:\n",
    "\n",
    "Train the model on batch 1, save the model checkpoint.\n",
    "Load the model checkpoint from step 1, train on batch 2, and save the updated checkpoint.\n",
    "Repeat this process sequentially for all batches.\n",
    "This approach tries to mitigate the GPU memory limitation by only keeping the model, one batch of data, and its computation graph in memory at a time, saving checkpoints after each batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUSTOM TRAINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T08:19:26.342582Z",
     "iopub.status.busy": "2024-12-26T08:19:26.342275Z",
     "iopub.status.idle": "2024-12-26T08:19:26.488145Z",
     "shell.execute_reply": "2024-12-26T08:19:26.487459Z",
     "shell.execute_reply.started": "2024-12-26T08:19:26.342548Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class CustomTrainer(Trainer): \n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Ensure class_weights is a tensor\n",
    "        if class_weights is not None:\n",
    "            self.class_weights = class_weights.clone().detach().to(self.args.device) ### Here !!start --->\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Extract labels and convert them to long type for cross_entropy\n",
    "        labels = inputs.pop(\"labels\").long()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # Extract logits assuming they are directly outputted by the model\n",
    "        logits = outputs.get('logits')\n",
    "\n",
    "        # Compute custom loss with class weights for imbalanced data handling\n",
    "        if self.class_weights is not None:\n",
    "            loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINGING ARGUMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='/kaggle/working/training_metadata',\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    save_steps=70,  # Save the model every 500 steps\n",
    "    save_total_limit=2,  # Only keep the last 2 checkpoints\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir='logs',\n",
    "    logging_steps=70,  # Log training metrics every 100 steps\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  LATEST CHECKPOINT FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T08:19:29.422170Z",
     "iopub.status.busy": "2024-12-26T08:19:29.421831Z",
     "iopub.status.idle": "2024-12-26T08:19:29.426433Z",
     "shell.execute_reply": "2024-12-26T08:19:29.425625Z",
     "shell.execute_reply.started": "2024-12-26T08:19:29.422142Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_latest_checkpoint(checkpoint_dir):\n",
    "    if os.path.exists(checkpoint_dir):\n",
    "        checkpoints = [os.path.join(checkpoint_dir, d) for d in os.listdir(checkpoint_dir)]\n",
    "        checkpoints = sorted(checkpoints, key=os.path.getmtime, reverse=True)\n",
    "        return checkpoints[0] if checkpoints else None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACTUAL FINE TUNING BEGINS !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. SPLITTING THE DATASET INTO CHUNKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T08:19:34.155485Z",
     "iopub.status.busy": "2024-12-26T08:19:34.155157Z",
     "iopub.status.idle": "2024-12-26T08:19:34.174275Z",
     "shell.execute_reply": "2024-12-26T08:19:34.173468Z",
     "shell.execute_reply.started": "2024-12-26T08:19:34.155458Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# Split the dataset into smaller batches\n",
    "batch_size = 512 # Number of examples per batch\n",
    "train_dataset = tokenized_datasets['train']\n",
    "\n",
    "# Create chunks of the dataset\n",
    "num_batches = len(train_dataset) // batch_size + int(len(train_dataset) % batch_size > 0)\n",
    "batches = random_split(train_dataset, [batch_size] * (num_batches - 1) + [len(train_dataset) % batch_size])\n",
    "\n",
    "print(f\"Number of batches: {num_batches}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. TRAINING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T08:19:35.971991Z",
     "iopub.status.busy": "2024-12-26T08:19:35.971676Z",
     "iopub.status.idle": "2024-12-26T08:19:35.976765Z",
     "shell.execute_reply": "2024-12-26T08:19:35.975947Z",
     "shell.execute_reply.started": "2024-12-26T08:19:35.971947Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_on_batch(model, batch, batch_id, output_dir, training_args):\n",
    "    print(f\"Training on batch {batch_id}...\")\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=batch,  # Train only on this batch\n",
    "        eval_dataset=tokenized_datasets['val'],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collate_fn,\n",
    "        compute_metrics=compute_metrics,\n",
    "        class_weights=class_weights,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the checkpoint\n",
    "    checkpoint_dir = os.path.join(output_dir, f\"batch_{batch_id}\")\n",
    "    trainer.save_model(checkpoint_dir)\n",
    "    print(f\"Batch {batch_id} training complete. Checkpoint saved at {checkpoint_dir}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. LOADING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T08:19:45.469336Z",
     "iopub.status.busy": "2024-12-26T08:19:45.469024Z",
     "iopub.status.idle": "2024-12-26T08:19:45.474428Z",
     "shell.execute_reply": "2024-12-26T08:19:45.473599Z",
     "shell.execute_reply.started": "2024-12-26T08:19:45.469312Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_model(latest_checkpoint, quantization_config, lora_config):\n",
    "    #print(f\"Loading model from: {latest_checkpoint or 'base model'}\")\n",
    "    model_name = latest_checkpoint if latest_checkpoint else \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "    # Load and configure the model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config,\n",
    "        num_labels=2\n",
    "    )\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "\n",
    "    # Initialize and configure the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Additional model configurations\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "    \n",
    "    print(\"Tokenizer configured successfully.\")\n",
    "\n",
    "\n",
    "    print(\"Model loaded and configured successfully.\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. TESTING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "def test_model(model,batch_size,output_test_dir):\n",
    "\n",
    "    # Define file paths for checkpoints and results\n",
    "    checkpoint_path = os.path.join(output_test_dir, 'model_checkpoint')\n",
    "    checkpoint_index_file = os.path.join(checkpoint_path, 'checkpoint_index.txt')\n",
    "    output_file_path_np = os.path.join(checkpoint_path, 'model_outputs.npy')\n",
    "    output_file_path_csv = os.path.join(checkpoint_path, 'model_outputs.csv')\n",
    "    predictions_file_path_csv = os.path.join(output_test_dir, 'predictions_with_logits_without_finetune.csv')\n",
    "    \n",
    "    # Create checkpoint directory if it doesn't exist\n",
    "    os.makedirs(checkpoint_path, exist_ok=True)\n",
    "    \n",
    "    # Convert summaries to a list\n",
    "    sentences = df_test['body'].tolist()\n",
    "    \n",
    "    # Initialize variables\n",
    "    start_index = 0\n",
    "    all_outputs = []\n",
    "    \n",
    "    # Load checkpoint if it exists\n",
    "    if os.path.exists(checkpoint_index_file):\n",
    "        with open(checkpoint_index_file, 'r') as f:\n",
    "            start_index = int(f.read().strip())\n",
    "        print(f\"Resuming from index {start_index}\")\n",
    "    \n",
    "    # Load previous outputs if they exist\n",
    "    if os.path.exists(output_file_path_np):\n",
    "        all_outputs = np.load(output_file_path_np, allow_pickle=True).tolist()\n",
    "        print(f\"Loaded previous outputs with shape {np.array(all_outputs).shape}\")\n",
    "    \n",
    "    # Process the sentences in batches\n",
    "    for i in range(start_index, len(sentences), batch_size):\n",
    "        # Get the batch of sentences\n",
    "        batch_sentences = sentences[i:i + batch_size]\n",
    "    \n",
    "        # Tokenize the batch\n",
    "        inputs = tokenizer(batch_sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "        # Move tensors to the device where the model is (e.g., GPU or CPU)\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "        # Perform inference and store the logits\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits.cpu().numpy()  # Move to CPU and convert to numpy array\n",
    "            all_outputs.extend(logits)  # Use extend to avoid adding a new dimension\n",
    "    \n",
    "        # Save the outputs and checkpoint periodically\n",
    "        if (i // batch_size) % 10 == 0:  # Save every 10 batches (adjust as needed)\n",
    "            all_outputs_np = np.array(all_outputs)\n",
    "            np.save(output_file_path_np, all_outputs_np)\n",
    "            df_outputs = pd.DataFrame(all_outputs_np)\n",
    "            df_outputs.to_csv(output_file_path_csv, index=False)\n",
    "            with open(checkpoint_index_file, 'w') as f:\n",
    "                f.write(str(i + batch_size))  # Update to the next starting index\n",
    "            print(f\"Checkpoint saved at index {i + batch_size}\")\n",
    "    \n",
    "    # Save final results\n",
    "    all_outputs_np = np.array(all_outputs)\n",
    "    np.save(output_file_path_np, all_outputs_np)\n",
    "    df_outputs = pd.DataFrame(all_outputs_np)\n",
    "    df_outputs.to_csv(output_file_path_csv, index=False)\n",
    "    print(\"Final results saved.\")\n",
    "\n",
    "    df_test['logits'] = [list(logit) for logit in all_outputs_np]\n",
    "    predictions = np.argmax(all_outputs_np, axis=1)\n",
    "    category_map = {0: 0, 1: 1}  # Adjust this mapping if needed\n",
    "    predictions = [category_map.get(pred, pred) for pred in predictions]      \n",
    "    df_test['predictions'] = predictions\n",
    "    \n",
    "    # Check if the length of predictions matches df_test\n",
    "    if len(all_outputs_np) == len(df_test):\n",
    "        df_test.to_csv(predictions_file_path_csv, index=False)\n",
    "        print(df_test[['logits', 'predictions']].head())\n",
    "        \n",
    "    else:\n",
    "        print(f\"Length mismatch: len(all_outputs_np) = {len(all_outputs_np)}, len(df_test) = {len(df_test)}\")\n",
    "\n",
    "    # Assuming you have true labels\n",
    "    true_labels = df_test['target']  # replace with actual labels\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    report = classification_report(true_labels, predictions)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Classification Report:\\n{report}\")\n",
    "\n",
    "    output_file = os.path.join(output_test_dir, 'test_metrics.txt')\n",
    "    \n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(f\"Accuracy: {accuracy}\\n\")\n",
    "        f.write(\"Classification Report:\\n\")\n",
    "        f.write(report)\n",
    "    \n",
    "    print(f\"Accuracy and classification report saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRIVER FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def driver(to_train,to_test):\n",
    "\n",
    "    # Define directories and configurations\n",
    "    model_dir = '/kaggle/input/final-model32/final_model'\n",
    "    output_train_dir = '/kaggle/working/train_data/model_checkpoint'\n",
    "    output_test_dir = '/kaggle/working/test_data'\n",
    "    \n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=8,\n",
    "        target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "        lora_dropout=0.05,\n",
    "        bias='none',\n",
    "        task_type='SEQ_CLS'\n",
    "    )\n",
    "    \n",
    "    \n",
    "    latest_checkpoint = get_latest_checkpoint(model_dir)\n",
    "    print(latest_checkpoint)\n",
    "    \n",
    "    # Load the model\n",
    "    model = load_model(latest_checkpoint, quantization_config, lora_config)\n",
    "    \n",
    "    \n",
    "    if to_train:\n",
    "         train_on_batch(model, batches[8], 8, output_train_dir , training_args)\n",
    "    \n",
    "    if to_test:\n",
    "        test_model(model,32,output_test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "driver(False,True)\n",
    "\n",
    "# def driver(to_train,to_test):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Data vs. Chunked Data: (ANALYSIS OF OUR APPOROACH)\n",
    "### Full Data: \n",
    "When the model is trained on the full dataset, the optimizer updates the model weights based on gradients computed over mini-batches. However, since the entire dataset is shuffled and seen multiple times (in epochs), the model gets repeated exposure to all parts of the data distribution, ensuring that the weights converge to a solution that reflects the entire dataset.\n",
    "### Chunked Data: \n",
    "When you train sequentially on data chunks, the model weights are updated based only on the current chunk. If the data in the chunks are not representative of the full dataset (e.g., they have a different distribution), the updates from the current chunk may overwrite or bias the knowledge learned from earlier chunksall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6366619,
     "sourceId": 10287653,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6376685,
     "sourceId": 10302064,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6376840,
     "sourceId": 10302274,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6378207,
     "sourceId": 10304072,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
